mongo "mongodb://m123-rs1-shard-00-00-glprg.mongodb.net:27017,m123-rs1-shard-00-01-glprg.mongodb.net:27017,m123-rs1-shard-00-02-glprg.mongodb.net:27017/test?replicaSet=m123-rs1-shard-0" --ssl --authenticationDatabase admin --username khylo --password <PASSWORD>

mongo "mongodb://cluster0-shard-00-00-jxeqq.mongodb.net:27017,cluster0-shard-00-01-jxeqq.mongodb.net:27017,cluster0-shard-00-02-jxeqq.mongodb.net:27017/test?replicaSet=Cluster0-shard-0" --authenticationDatabase admin --ssl --username m001-student --password m001-mongodb-basics

mongorestore --drop --host $hostname --port 37017 --authenticationDatabase $authdb --ssl --username $username --gzip --password $password --dir mongodump-people-signup-score

Local Import
	mongoimport --db capc --collection RegisteredUser --drop --file ./jobs/data/users.json --jsonArray
	
	mongoimport --drop -d students -c grades grades.json
	
	
	mongorestore (restore from mongodump)
	
	mongotop  // top runnign jobs in monfo
	
	mongostat    / /statistics.. inserts queries updates or deletes
	
	//Shutdown mongod from mongo
	mongo admin --eval 'db.shutdownServer()'
	
	mongod  --fork --logpath [path[ .. start as a background process.. must specify logpath as well
	
	mongod --config /etc/mongod.conf
	
Loopback.
	look in server/datasources.json for db config. e.g.g mongo
	
CopyCollection
	from within mongod
		db.myoriginal.aggregate([ { $match: {} }, { $out: "mycopy" } ])
		db.movieDetails.aggregate([ { $match: {} }, { $out: "movie2" } ]) 
		
	from command line
		mongodump -d db -c sourcecollection 
		mongorestore -d db -c targetcollection --dir=dump/<db>/<sourcecollection.bson>
		
		db.collection.help().. show all commands


		show useres   
			use admin
			db.system.users.find()
		
find
*Mongod will by default log slow queries. So shoudl set some automation up to find these in the logs and notify.*

	db.<collection>.find(<query>, [projection]).sort()
	Note can also append .explain() at end of query to get explainPlan
		query = { $<Operator(s)>  $gt, $lt, $gte, $in
				And used by default.
			db.inventory.find( { status: "A", qty: { $lt: 30 } } )
			
		Projections used to specify columns, This only returns _id and name
			coll = db.users; // example of shortcut using variables
			coll.find( { }, { name: true } )
		
		for or use
			db.inventory.find( { $or: [ { status: "A" }, { qty: { $lt: 30 } } ] } )
			db.inventory.find( { $or: [ { status: "A" }, { qty: { $lt: 30 } } ] } ).explain()
		
		Text find				
			db.product.find({$text:{$search: "My search string"}))
			
			Text find ORs search strings together so may give us unexpected results, so shoudl use
			db.product.find({$text:{$search: "My search string"}, {"score":{$meta:"textScore"}}).sort({score:{$meta:"TextScore"}})
			
		//Find countries with 2nd name Sweden
		db.movieDetails.find({'countries':'Sweden'}).count()
		
		**Note if you are filtering on 2 values of one field then be careful
		**Wrong
		db.movie2.find({"imdb.votes":{"$lt":10000}, "year":{"$lte":2013}, "year":{"$gte":2010}},{title:1, year:1, _id:0, "imdb.votes":1})
		This will only filter on year gte 2010 since it overrides the 2013
		**Right
		db.movie2.find({"imdb.votes":{"$lt":10000}, "year":{"$lte":2013, "$gte":2010}},{title:1, year:1, _id:0, "imdb.votes":1})

	Sort
		db.grades.find({type : "homework"}).sort({'student_id':1, score:1})  sort by student, then score after filtering only homework 'type'
		db.grades.find( { }, { 'student_id' : 1, 'type' : 1, 'score' : 1, '_id' : 0 } ).sort( { 'student_id' : 1, 'score' : 1 } ).limit( 5 )
	

	Explain
	After 3.0 format changed from db.people.find().explain
	to
	db.people.explain(). ...   find(), upother way preferreddate(),  aggregate , help      not insert
		return explainable object

	old format still works but first find() returns cursor   (can run next on this)  .. but 
	"allPlansExecution"

	Covered Query
		Query that can be fulfilled enturely by index.. e..g find must match index (eg. must project only index fields, or subset)
		much faster, only need to examine 1 key in index.. Noramlly 0 docs examined
		normally have to filter out _id field
	

		
		
	Index
	choosing index

		Mongo will identify some canditate indexes.
		Then run n theads in parallel doing the n qeries, and see which returns quicker (maybe simpler subset of query)
		Winning index is cached for future use
			indexes cache reevaluated afer
			Writes no longer do this .. After 1000 writes this may be re-evalued 
			Instead, we evict when the "works" of the first portion of the query exceed the number of "works" used to decide on the winning plan by a factor of 10x
			rebuild index
			Add or reomove another index
			or after restarat
			
    Need to keep Indexes in memory
		Size ?
			db.people.stats()

		make sure index can fit in memory for Perf reasons
	
		List
			db.stiudents.getIndexes();
		Delete	use key from list above
			db.students.dropIndex({'student_id':1}) // Can't delete _id
			
		Create
			db.student.ensureIndex({'teachers':1})
			
			db.student.createIndex({'teachers':1, 'class':-1})
			db.student.createIndex({'teachers':1, 'class':-1}, {unique:true})  // Can enforce unique/distinct values. Dupicate will cause error  Do getIndexes() to see if unique or not (_id is anomoly)
		
		verify
			db.students.explain().find({'teachers':{$all:[0,1]}})   
			
			Can put explain(true) to run query and get more info
				uses BtreeCursor Index for lookup
				
		for compound index
			e.g. surname, firstname, dob
				we can use this compound index to lookup on surname alone, then do full table scan for DOB
				but we can't lookup DOB o9r firstname alone. 
				
			Adding indexes makes writes slower. Pattern. Before large insert have no indexes. Add indexes after large write.
			

		MultiKey Index (e.g. on array)
			Can't have index on more than 1 field is array
			db.foo.createIndex( { a:1, b:1 } )
			
			but can create for 1 array per document 					
					Index expands all entries in collection to index, and maps them to other values in the index
					e.g.
						{a: 1, b:[1,2,3]}   ok
						{a:[2,3], b:5}		ok
						{a:[2,3], b:[5,4]}  Not Ok.. 2 arrays for field with index
						
					
			So if we add elements and make some fields arrays this may break indexes
			
		Sparse index .. can make a unique index on field that is not always present
			e.g.
		db.people.createIndex({'sparseField':1},{'unique':true, 'sparse':true})
		
		Regex
			If searching for regex, can only use Index iff searching for starts with  (/^searchString/ ) e.g.
			db.useres.find({username: /^kirby/})


		can create index in foreground or bacgrund

		forgound defalt.. blocking call, block DB and collection
			BLOCKS readers and writers to whole DB (... eg. use db)


		background
			slower
			but doesn't block queries (does block mongo shell, but other instances not blocked)
		db.people.createIndex({'sparseField':1},{'background':true})

		create index on different server first (out of mongo replica set)
			run index creation in foreground
			bring back into set
			it will spread?

			
		For indices subelements use dot notation, 
		
			db.people.createIndex({'work_history.company':-1})
			
			
	db.movieDetails.findOne
	or
	db.movieDetails.find
	Array
		Query family in list of genres
		> db.movieDetails.find({"genres":"Family"}).count()
		124
		Query family in 2nd position..
		> db.movieDetails.find({"genres.1":"Family"}).count()
		58
		
	Q. 2.7 (extra non homework)
		db.createCollection("testMovie")		
		db.testMovie.insert( [ {  some test film output from movieDetails, and change awards }])
		db.testMovie.find({"awards.oscars.award":{$eq:"bestPicture"}}).count()
		
		
		
	Aggregation
			db.grades.aggregate([{'$group':{'_id':'$student_id', 'average':{$avg:'$score'}}}, 
									{'$sort':{'average':-1}}, {'$limit':1}])	
		
		
	Assignment
		var myDoc = db.movieDetails.find({"imdb.id":"tt4368814"})

update.
	updateOne
	updateMany

	upsert

	> db.movieDetails.updateOne({title:"MyTestFilm"},{$set:{"title" :"MyTestFilm", "year" : 1982, "rated" : "PG", "runtime" : 113, "countries" : [ "USA" ], "genres" : [ "Action", "Adventure", "Drama" ], "director" : "Nicholas Meyer", "writers" : [ "Gene Roddenberry", "Harve Bennett", "Jack B. Sowards", "Jack B. Sowards" ], "actors" : [ "William Shatner", "Leonard Nimoy", "DeForest Kelley", "James Doohan" ], "plot" : "With the assistance of the Enterprise crew, Admiral Kirk must stop an old nemesis, Khan Noonien Singh, from using the life-generating Genesis Device as the ultimate weapon.","imdb" : { "id" : "tt0084726", "rating" : 7.7, "votes" : 86687 }, "metacritic" : 71, "awards" : { "wins" : 2, "nominations" : 9, "text" : "2 wins & 9 nominations." }, "type" : "movie" }},{upsert:true})
	
	$push - adds items in the order in which they were received. Also you can add same items several times
	$addToSet - adds just unique items, but order of items is not guarantied
	
Replace
	ReplaceOne
	replaceMany
	
Delete
	deleteOne
	deleteMany
	
	
	Scaling Horizintally. Sharded clusters
	
		mongos (routeres).. used for handling sharded clusters. Which is multiple replica sets (up to 12 at time of writing).
		when connecting to a mongos router.
			db.mongos.find({},  {_id:1})
			sh.status()
			sh.enableSharding("Test") // enable partiition is now true
			
		create shard key
			One per collection.. can't be changed once its in place
			https://docs.mongodb.com/manual/tutorial/choose-a-shard-key/?_ga=2.220197530.2030665551.1512995876-1430811341.1509531785
		
	
	Scaling veritically. 
		Rebuilds each component one by one.  to more powerful servers. e.g. moving from M10 cluster to M60 cluster.
	
	
	id  c
	00  54
	01  52
	10  22
	11 97
Aggregation
	Like groupBy and having
	Aggregation uses pipline.. filter / aggregate / sort ... steps within array
		
	Stages of pipeline
	
		$project  (select sub elements.. reshaping of document  1 doc -> 1 doc)
			$toUpper
			$toLower
			$multiply
		$match   filter  n: 1 ..   $gt $lt etc
		$group - aggregate .. sum / count .. n:10
					$sum
					$avg
					$min
					$max
					$push   push to array (no duplicate check )
					$addToSet   add to array, e.g. pull out list of categories for a group.. With Duplicate check
					$first   ... can use after sorting if you want the first`
					$last   .. .after sorting
		$sort    1:1  100MB limit for in-memory sort.. Can be done before or after grouping
		$skip   skips  n:1 only makes sense after sort
		$limit   limit  n:10 only makes sense after sort AND after skip
		$unwind - normalise data  1:n  e.g. array of tags   .. explodes data so we can use $group on it.
		$out   output
		$redact   security related to reduce docs useres see   n:1
		$geonear   reduce based on location  n:1
		
		
	** Examples
	// Group by manufactere (which is key on docs)
	db.products.aggregate(
	[{
		$group:{ _id:"$manufacturer", num_products: {$sum:1}	}
	}])
	
	//Can have useful name in _id if we want more clarity
	db.products.aggregate(	[{		$group:{ _id:{"maker":"$manufacturer"}, num_products: {$sum:1}	} 	}])
	
	//Can sum fields
	db.products.aggregate(	[{		$group:{ _id:{"maker":"$manufacturer"}, num_products: {$sum:$price}	} 	}])
	
	Avg
	db.zips.aggregate([{$group:{_id:"$state", "average_pop":{$avg:"$pop"}}}])
	
	Max
	db.zips.aggregate([{$group:{_id:"$state", "pop":{$max:"$pop"}}}])
	
	AddToSet
	db.zips.aggregate([{$group:{_id:"$city", "postal_codes_per_city":{$addToSet:"$_id"}}}])
	//Problem with above  is differnt cities with the same name but in different states are combined
	db.zips.aggregate([{$group:{_id:{City:"$city",State:"$state"}, "postal_codes_per_city":{$addToSet:"$_id"}}}])
	
	Double Group By  (Note 2nd is taking part of the key of the first)
	db.fun.aggregate([{$group:{_id:{a:"$a", b:"$b"}, c:{$max:"$c"}}}, {$group:{_id:"$_id.a", c:{$min:"$c"}}}])
	
	//Compund _id key in order to group by more than one field..
	db.products.aggregate(
	[{
		$group:{ _id:{"manufacturer":$manufacturer, "category", "$category"}, num_products: {$sum:1}	}
	}])
	
	//Project:
		db.zips.aggregate([{$project:{
			"_id":0,
			"city":{$toLower:"$city"}, 
			"pop":"$pop",
			"state":"$state",
			"zip":"$_id"
		}}])
		
		or
		
		db.zips.aggregate([{$project:{
			"_id":0,
			"city":{$toLower:"$city"}, 
			"pop":1,
			"state":1,
			"zip":"$_id"
		}}])
		
	// Match
	db.zips.aggregate([{$match:{state:"CA"}}])
	db.zips.aggregate([{$match:{pop:{"$gt":100000}}}])
	
	Sort
	//Sort by state/ city
	db.zips.aggregate([{$sort:{state:1, city:1}}])
	//sort by pop desc
	db.zips.aggregate([{$sort:{pop:-1}}])
	
	// First and Las .. must do another group to get them
	db.fun.aggregate([
		{$match:{a:0}},
		{$sort:{c:-1}},
		{$group:{_id:"$a", c:{$first:"$c"}}}
	])
	
	All In one
	use zips;
	db.zips.aggregate([	
		{$match:{state:"CA"}}, 
		{$group:{_id:"$city", pop:{$sum:"$pop"}, zip:{$push:"$_id"}}},
		{$project:{_id:0, city:"$_id", pop:1, zip:1}},
		{"$sort":{pop:-1}}
		])
		
	db.zips.aggregate([
		{$match:     {     state:"NY"     }    },
		{$group:     {     _id: "$city",     population: {$sum:"$pop"},     }    },
		{$project:     {     _id: 0,     city: "$_id",     population: 1,     }    },
		{$sort:     {     population:-1     }    },
	])
	
	# Can connect to different servers in cluster from within mongo shell e.g. gconnect to secondary
	db = connect("127.0.0.1:27013/m201")
	sb.setSlaveOk()
	
	cd aggregation/
	type "first_phase3_m101p.js" | mongo
		
	Homework
	//Which author has made most comments
	//5.1
	use blog;
	db.posts.aggregate([{$unwind:"$comments"},{$group:{"_id":"$comments.author", "numComments":{$sum:1}}},{$sort:{numComments:-1}}])
	
	//5.2
	//Average population in cities in CA and NY with populations > 25000
	// Wrong doesn't add up population for city .. .db.small_zips.aggregate([{$match:{state:{"$in":["CA","NY"]}, pop:{$gt:25000}}},{$project:{_id:0,city:1,state:1,pop:1}},{$group: {_id:"$state","avg":{$avg:"$pop"}}}])
	// This is correct
	db.small_zips.aggregate([{$group: {_id:{state:"$state","city":"$city"}, pop:{$sum:"$pop"}}},{$match:{"_id.state":{"$in":["CA","NY"]}, pop:{$gt:25000}}},{$project:{_id:0,city:"$_id.city",state:"$_id.state",pop:1}},{$group: {_id:"$state","avg":{$avg:"$pop"}}}])
	
	//5.3
	//average per studentPerClass to get average student score per class, then average per class, then sort
	db.grades.aggregate([{$unwind:"$scores"},{$match:{$or:[{"scores.type":"exam"},{"scores.type":"homework"}]}}, {$group:{"_id":{"student":"$student_id","class":"$class_id"}, avgPerStudentPerClass:{$avg:"$scores.score"}}}, 
		{$group:{"_id":"$_id.class", avgPerClass:{$avg:"$avgPerStudentPerClass"}}}, {$sort:{avgPerClass:-1}}])
		
	//5.4
	//
	use zips
	db.zips.aggregate([  {$project:     {    first_char: {$substr : ["$city",0,1]}, pop:"$pop"    }   },
						{$match:{first_char:{$in:["B","D","O","G","N","M"]}}},
						{$group:{"_id":null, total:{$sum:"$pop"}}}
		])
	
	
Replication and Sharding
	Writing
		Journal log of all events. Stored in memory and written to disk
		Journal write to disk is the main write
		
		by default normal write has w:1 and j:00
			this mean s that write is persisted but journal isn't by time of response.
			
			
				w | j  
		default 1 | 0    fast but small window where j is not written and data not fully persisted (replica set?)  
	            1 | 1  slower but safer
				0       very fast but no guarantees. Not recommended
				
				
			Network failures compound things.
				Inserts generally safe... 
				updates les so if using $sum etc. Safer to do a read first and then delete, and insert updated value
				
	Replication
		used for availibility 
		
		use rs commands
		rs.status()
		rs.initialise()
		rs.conf()
		rs.help()
		
		Op log.
			Primary writes to oplog (idempotent meaning can rerun tasks without fear)
			oplog gets asynchronously replicated to secondarys
			oplog uses capped collection.. A fixed-sized collection that automatically overwrites its oldest entries when it reaches its maximum size. The MongoDB oplog that is used in replication is a capped collection.
			
			to see opLog
			
			use local;
			db.oplog.rs.find().pretty()
			
			op .. i = insert,  c = create (collection), etc
			Db keeps track of last time it read and updated from opLog
				rs.status(0 shows optIme, and optimeDate so DB knows what last update was)
				
			If oplog rolls over (past cap) before secondary has written .. Can still do replication but much slower.. Needs to copy entire data set from new primary
			
		Replica Set  ... minimum 3 nodes to ensure election of new primary and to have redundancy
			Always write to primary
			
		Election
			Node types
				Normal 
				Arbiter (used only for voting .. e.g to maintain majority)
				Delayed / Regular .. can't vote. can't be pimary node  P=0
				Hidden .. cant be primary. Can Vote.. . Used to analytics      P=0
				
		
		rs.initiate(config);
		rs.status();
		
		** If you want to read on secondary
		rs.slaveOk()
				`
	Shrading
		(**Pattern, create compound shard Key with _Id appended to normally slected key.)
		Shard a collection accross shards. 
			used for horizontal scaling, but aloso can be used to increase performance, allowing parallelization of reads/ writes
		Chunks are mapped to shards
		Config server maps chunks to shards. mongoS checks against config servver to find the shard. Recommend 3 config servers for prod system (seems to be shard size independent)
		
		Chunks are Range based or hash based
			As keys are added to chunks, chunks can be split, but only if range allows. Can end up with jumbo chunks if we can split chunk key smaller. In this case creating compound shard Key would help (e.g. appending _id to key)
			
		Ordered bulk writes slow down a lot in shard cluster (executed serially)
		Un Ordered bulk writes speed up a lot in shard cluster (executed in parallel)
		
		Examples
			sh.shardCollection('m210.peopl', {last_name:1})
			// divides range into chunks
			// chunks max size 64MB
		
		every doc MUST contain shard Key, otherwise wont know which server to put doc. Shard key is  indexed automatically once shard is created,. but does not have to be unique
			shardKey is immutable
			shardKey is not unique
			must have an index starting with shardKey.. Could be more than just shardKey (e.g.g student_id plus class)
				Can't be multiKey Index
			Update   ..must specify shardKey or multi .. 
			No shardKey in query means all RS in shard get query.
			No unique Key unless starts with shardKey
			
		Types of read..
			Scatterd Gather queries.. Talks to all shard nodes.. Not good
				results in local sort / skip/ limit etc in each shard before final sort/ skip / limit in primary node of cluster
			Routed Quesries
		
		Normally run mongos on App server since it is lightweight.
			Normally have multiple mongos on app server for redundancy
			Driver handles connecting to appropriuate mongos if mongos's go down
			
		Choosing shard key	
			`Sufficient Cardinality (enough values to spread out accross shards)
			Frequency .. want even distribtion of values
			Rate of Change
			Be careful of always increasing values (e.g. bsonId).. When you define shard values might use max value.. But after time all values will tend to always hit last shard since they go aboove highest
			Make compound key to avoid jumbo chunks
			Can have monotonicall increasing or decreasing id in compund shard key as long as it is not first value.. 
				
		Write Consistency
			Writes only occur to Primary if primary is down then no writes until new primary is elected) (Normally failover approx 3 secs)
			Read can occur from primary and secondary, BUT if you want read after write consiistency you must read from primary.
			
			Consider write concerns with sharding
				w="majority" means majority of RS's in possibly each shard need to respond to mongos
				
		Read From secondary (main use is high availability, not perf)
			Off by default				
				readPref can be Primary, primaryPreferred, Secondary, secondaryPreferred, Nearest
				e.g.g db.col.find().readPref("nearest")
			Can lead to stale data been read
			
			Good for:
				Analystics reporting jobs, which might have heavy reads and may effect perf
				Keep app data reading from primary
				Local reads in geographically distributed datasets e.g. 1 primary, and 1 secondary in locale1, 1 secondary in locale 2. 
				
			Bad for
				In general, bad idea
				Proviuding extra capacity for reads... Shoudl use sharding for this  Secondarys are getting as much write information as Primary due to replication, so reading from secondary does not help.
				*Never* read from secondary in sharded cluster. You should never read from a secondary on a sharded cluster. When you do this you might get stale results with missing or duplicated data because of incomplete or terminated chunk migrations. Shoudl always go through mongos not directly to shard
				
		Can have specific indexes on secondary nodes (e..g ) if used specificalyl for analytics.
			Need to make sure this node does not become primary (priority =0, hidden node, or delayed secondary)
			
			
		Creating Replica Sets (on one server not normal)
			mkdir -p /data/rs1 /data/rs2 /data/rs3
			mongod --replSet m101 --logpath "1.log" --dbpath /data/rs1 --port 27017 --oplogSize 64 --fork --smallfiles
			mongod --replSet m101 --logpath "2.log" --dbpath /data/rs2 --port 27018 --oplogSize 64 --smallfiles --fork
			mongod --replSet m101 --logpath "3.log" --dbpath /data/rs3 --port 27019 --oplogSize 64 --smallfiles --fork
			
			** Login to mongo shell  (NOT on 27017.. Can't initialise on host that can't become primary, which in this case 27017 can't)
			config = { _id: "m101", members:[
					  { _id : 0, host : "localhost:27017"},
					  { _id : 1, host : "localhost:27018"},
					  { _id : 2, host : "localhost:27019"} ]
			};
			
			e.g. see 
			REM  connect to one server and initiate the set
			mongo --port 37017 --eval "config = { _id: 's0', members:[{ _id : 0, host : 'localhost:37017' },{ _id : 1, host : 'localhost:37018' },{ _id : 2, host : 'localhost:37019' }]};rs.initiate(config)"
			
			# set up config servers (using mongos)
			# Start mongos and tell it about config servers
			start mongos  --configdb csReplSet/localhost:57040,localhost:57041,localhost:57042
			REM  add shards and enable sharding on the test db
			mongo --eval "db=db.getSisterDB('admin');db.runCommand( { addshard : 's0/'+'localhost:37017' } );db.runCommand( { addshard : 's1/'+'localhost:47017' } );db.runCommand( { addshard : 's2/'+'localhost:57017' } )"
			mongo --eval "db=db.getSisterDB('admin');db.runCommand({enableSharding: 'school'});db.runCommand({shardCollection: 'school.students', key: {student_id:1}})"


				sh.status()

			
		To createIndex from Secondary (usefiul for analytics / reporting/ text search). Should not become primary
			Bring down cluster
			Bring up secondary in standalone mode (same dbpath)
			Then run db.retaurants.createIndex({"cusine":1})  # Now this index wont get propogated. Therefore need to make sure this secondary does not become primary.. 
		
		Aggregation		
			When aggregating on shard server will determine if it can optimise to a single shard or if it has to query all shards. 
			If more than one shard involved then there also needs to be  final collection, and ordering of results. Normally this occurs on a random shard node, but if some commands used, e.g. then it will always occur on primary shard which can increase workload on that shard node.
				$out
				$facet
				$lookup
				$graphLookup
			
			Aggregation optimizations
				matches will be moved before sorts, in order to reduce data accross nodes
				limits and skips are automatically ordered so that limit is first, skip is second.. (combines so that limit may chnage in order to keep symantics correct)
			
		
		Failover and ROLLBACK
			if primary goes down. When it comes back up it will cehck off current Primary for writes.
			if original primary had writes that new primary never got, then rollback writes that original had. (Produces rollback file that can be applied manually)
			
			If we set write concern to 'majority' then this shoudln't happen (might be edge cases)
		
				
***
Performance	 .. https://university.mongodb.com/mercury/M201
***

	in Memory actions
		aggregation/ index traversing / write operations/ query engine / connections
		
	Hardware	
		
		Recommend RAID 10    (do not recommend RaID 0 / 5 /6  reduced performace)
					raid 10		=     raid 0
							   raid 1			raid 1
							   
		Can use multiple Disks. .allows IO load to be distributed and parallelized
		
		Wired Tiger is more CPU intensive but is the future
		
		
		mongod --directtoryperdb   this creates dir per db
			Allows these to be symbolic links and have these mapped to different drives since this can increase parallelism
			
	Indexes
	
		db.currentOp()    // show current running process e.g.g background index
		db.killOp() // e.g. kill background Op.
	
		Single Field Index
			db.people.createIndex({'ssn':1})
			
			Dont create index on sub document. Instead use dot notation to specify an elemnt of subDocument. Indexing on Docuemnt ineffefient if its a big Document
			
		Compount Index
			On multiple fields.   
			Rule of thumb  Equality  - Sort - Range
				Equality field before sort field before range field
			
			
		MultiKey Index e.g. on Arrays.. Creates Index key for each element in array. Be careful if using large arrays
			Can't use compund fields where more than one field is an array (per document)
			
		Text Index	  Pass in text keyword to help avoid collection scan. Creates index for every unique word in string. (spaces and hypens are tokenizer)
			db.product.createIndex({productName:"text"})  
			
			for searching use
			db.product.find({$text:{$search: "My search string"}))
		
		Partial Index .. used to save space if it makes sense. .Partial Index preferred over sparse
		 (General case of Sparse Index).. Can't have both sparse and Partil Key
			Can't have shard index as partial index
		 Be careful.. Find predicate must match partial index
		 // rerun the query (doesn't use the partial index) see performance/parial_index.js examples
			db.restaurants.find({'address.city': 'New York', 'cuisine': 'Sichuan'})

			// adding the stars predicate allows us to use the partial index
			exp.find({'address.city': 'New York', cuisine: 'Sichuan', stars: { $gt: 4.0 }})
	Explains
		Can create Explainable Object e.g.
			exp=db.people.explain("executionStats")  attche explain to collection so it gets run every time
		db.people.find({'ssn':"720-38-5636"}).explain("executionStats")
		
	WriteConcerns
		Can set these on Connection.. Set them on collection in driver... (similar) .. can set them in the replicaSet (by DBA). Probably DBa preferred
		w: 1 .. majority number of nodes to ackknowledge write
		j: false  //wait for journal to be written to On Primary
		wtimeout: 5 // ms   how long to wait for acknowledgement
		
		e.g.g {w:"majority", j:"true"}  w = majority shoudl remove most rollbacks.. Shoudl be 2ndary with writes acknowledged shoudl become primary in failover
		
	
Read Preferences
		Reads and writes goto primary by defalt. But can change
		
		Reads can be set to read from secondary using read prefernce
		
		primary
		primary preferred
		secondary
		secondary preferred
		nearest
		
		if readong from secondary then eventually consistent	
		
		
	Collations  See performance\collations.js   
		Language Specific rules for text searches
		
		locale, caseLevel, caseFirst, strength, numericOrdering, alternate, maxvarialbe, backwards
		Allows locales, case insentive (strength:1) searches
		Collation appears in explain results
		Can create Indexes for individual collations, e.g. en or pt, or it
		
		db.createCollection( "foreign_text", {collation: {locale: "pt"}})
		// insert an example document
		db.foreign_text.insert({ "name": "MÃ¡ximo", "text": "Bom dia minha gente!"})
		// explain the following query (uses the Portuguese collation)
		db.foreign_text.find({ _id: {$exists:1 } } ).explain()
	
	
Profiling	have mongod log to db.system.profile
	mongod --profile [level] [--slowms 2]
	level
	0 default .. no logging
	1 log slow queries (can spcify --slowms )
	2 log all queries (maybe for dev)
	
	from mongo shell
	db.getProfilingLevel(	)
	db.setProfilingLevel(2)
	db.getProfilingStatus(	)
	db.setProfilingStatus(1,4) 		// Level 1:  4 ms
	
	Can then query profile table 
	 db.system.profile.find( { millis : { $gt:1000 } } ).sort( { ts : -1 } )
	 
Clustering
	Replication
		2 types Binary vs statemnt based
		
		Binary keeps binary copy of writes in binary log
			Pro, consise fast. less data
			Cons requires strict allignment of OS's, hardware arch, and DB versions
			
		Statement based	.. keeps statemnts in statement log
			Not stement log adopted slightly. e.g. increment will change to write exact value in statemnet log
			That menas we can rerunlogs multiple times without fear of corruption. (Idempotency)
			Send wr
			
	Replica set 
	 1 primary.. n-1 secondarys.  and or Arbiter.. used in voting. cannot become primary as it holds no data
	 Secondars updates asyncronous Pv1 (raft protocol) Default, Pv0
	 Ope log sync'ed using idempotent statemnt based replication
	 
	 Can have up to 50 members in replica set (e.g. for geographic distribtion).. only 7 votering members (only 7 can be primary)
	 
	 cAN HAVE DELAYED SECONDARY NODES E..G 2 HOURS.
		this means that if ther is a probem we have 2 hours to restore from delayed node before data gets propogated there
	 
	 Recommend NOT using Arbiters	
	 
	 HW
		sudo mkdir -p /var/mongodb/pki
		sudo chown vagrant:vagrant -R /var/mongodb
		openssl rand -base64 741 > /var/mongodb/pki/m103-keyfile
		chmod 600 /var/mongodb/pki/m103-
		mkdir /var/mongodb/db/1 /var/mongodb/db/2 /var/mongodb/db/3
		
		Setup users
		mongo admin --host localhost:27001 --eval '
		  db.createUser({
			user: "m103-admin",
			pwd: "m103-pass",
			roles: [
			  {role: "root", db: "admin"}
			]
		  })'
		  
		  mongo admin --host localhost:27001 --eval 'db.createUser({user: "m103-admin",	pwd: "m103-pass",	roles: [ {role: "root", db: "admin"}] })'
		
e.g. setup		
		config filename	mongod-repl-1.conf	mongod-repl-2.conf	mongod-repl-3.conf
		port	27001	27002	27003
		dbPath	/var/mongodb/db/1	/var/mongodb/db/2	/var/mongodb/db/3
		logPath	/var/mongodb/db/mongod1.log	/var/mongodb/db/mongod2.log	/var/mongodb/db/mongod3.log
		replSet	m103-repl	m103-repl	m103-repl
		keyFile	/var/mongodb/pki/m103-keyfile	/var/mongodb/pki/m103-keyfile	/var/mongodb/pki/m103-keyfile
		bindIP	localhost,192.168.103.100	localhost,192.168.103.100	localhost,192.168.103.100
		
		rs.initiate()
		rs.status()
	
	Homework
		Lab 2.1 got wrong. 
		Heres what I think is the right answer now (* marks where index used for both search and sort)
			When you see SORT_KEY_GENERATOR that is not using Index I believe
		
		var exp = db.people.explain("executionStats")
		exp.find({ "address.city": "West Cindy"}).sort({"address.city":-1})
		exp.find({ "first_name": "Jessica", "address.state": { $lt: "S"}}).sort({"address.city":1})
		*exp.find({ "address.state": "South Dakota", "first_name": "Jessica"}).sort({"address.city":-1})
		*exp.find({ "first_name": "Jessica"}).sort({"address.state":1,"address.city":1})
		exp.find({ "first_name": {$gt:"J"}}).sort({"address.city":-1})
		 
		 
		 Java
		db.posts.createIndex({date:-1})
		db.posts.createIndex({tags:1,date:-1})
		db.posts.createIndex({permalink:1})
		
		db.sysprofile.find({'op':'query',ns:'school2.students'}).sort({millis:-1 } ).limit(2).pretty()
		
		
	Sharding	
	 How horizontal scaling wokrs.. breaks up collection to logical hosts (RS).. shard Key used to split
	 How horizontal scaling wokrs.. breaks up collection to logical hosts (RS).. shard Key used to split
	 each shard is potentially s replicaSet
	 use mongos for sharding
	 
Examples

stop mongod
use admin
db.shutdownServer()
quit()

    Create user
mongo admin --host localhost:27000 --eval '
  db.createUser({
    user: "m103-admin",
    pwd: "m103-pass",
    roles: [
      {role: "root", db: "admin"}
    ]
  })
'
mongo --port 27000 ;--username m103-admin --password m103-pass -authenticationDatabase admin
use admin
db.createUser({user:'m103-application-user',
pwd:'m103-application-pass', roles:['readWrite'], db:'applicationData',)
mongo admin --host localhost:27000 --eval '
  db.createUser({
    user: "m103-application-user",
    pwd: "m103-application-pass",
    roles: [
      {role: "readWrite", db: "applicationData"}
    ]
  })
'
mongoimport --port 27000 -d applicationData -c products -u m103-application-user -p m103-application-pa
ss --authenticationDatabase=admin dataset/products.json


mongo --eval "load('products__m101.js')"		
=======
